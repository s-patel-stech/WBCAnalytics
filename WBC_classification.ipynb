{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "# no.of.trees for Random Forests\n",
    "num_trees = 100\n",
    "\n",
    "# bins for histogram\n",
    "bins = 8\n",
    "\n",
    "# train_test_split size\n",
    "test_size = 0.20\n",
    "\n",
    "# seed for reproducing same results\n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#print (\"TensorFlow version: \" + tf.__version__)\n",
    "# Note you no longer need to import keras, use tf.keras instead\n",
    "#import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!ls\n",
    "#import os\n",
    "#os.chdir('Blood count project/WBC TESTING/')\n",
    "#!ls\n",
    "\n",
    "#working\n",
    "#import tarfile\n",
    "#fname='Main Dataset.tar.gz'\n",
    "#if (fname.endswith(\"tar.gz\")):\n",
    "#    tar = tarfile.open(fname, \"r:gz\")\n",
    "#    tar.extractall()\n",
    "#    tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.5/dist-packages (4.2.0.32)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/sandeep/.local/lib/python3.5/site-packages (from opencv-python) (1.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mahotas in /home/sandeep/.local/lib/python3.5/site-packages (1.4.9)\n",
      "Requirement already satisfied: numpy in /home/sandeep/.local/lib/python3.5/site-packages (from mahotas) (1.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/sandeep/.local/lib/python3.5/site-packages (0.24.2)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/lib/python3/dist-packages (from pandas) (2014.10)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/sandeep/.local/lib/python3.5/site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/sandeep/.local/lib/python3.5/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sandeep/.local/lib/python3.5/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install mahotas\n",
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.5/dist-packages (4.2.0.32)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/sandeep/.local/lib/python3.5/site-packages (from opencv-python) (1.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install mahotas\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-1: Hu Moments\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-2: Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # compute the haralick texture feature vector\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    # return the result\n",
    "    return haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-3: Color Histogram\n",
    "def fd_histogram(image, mask=None):\n",
    "    # convert the image to HSV color-space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # compute the color histogram\n",
    "    #hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    hist = cv2.calcHist([image],[0,1,2],None,[8,8,8],[0,256,0,256,0,256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    # return the histogram\n",
    "    hist=hist.flatten()\n",
    "    import numpy as np\n",
    "    meanss=np.mean(hist)\n",
    "    stdss=np.std(hist)\n",
    "    #Combining means and std\n",
    "    #import numpy as np\n",
    "    #statss=np.concatenate([meanss,stdss]).flatten()\n",
    "    stats = np.array([meanss,stdss])\n",
    "    (means,stds)=cv2.meanStdDev(image)\n",
    "    #Combining means and std\n",
    "    import numpy as np\n",
    "    statss=np.concatenate([means,stds]).flatten()\n",
    "    result = np.hstack([statss,stats])\n",
    "    return hist.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "ppc = 16\n",
    "hog_images = []\n",
    "hog_features =[]\n",
    "def hog1(image):\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image=color.rgb2gray(image)\n",
    "    fd,hog_image = hog(image, orientations=8, pixels_per_cell=(ppc,ppc),cells_per_block=(4, 4),block_norm= 'L2',visualise=True)\n",
    "    #hog_images.append(hog_image)\n",
    "    hog_features.append(fd)\n",
    "    return hog_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    \"\"\"\n",
    "    Load the data and labels from the given folder.\n",
    "    \"\"\"\n",
    "    global_features = []\n",
    "    y = []\n",
    "\n",
    "    for wbc_type in os.listdir(folder):\n",
    "        if not wbc_type.startswith('.'):\n",
    "            #print(\"wbc_type \", wbc_type)\n",
    "            #if wbc_type in ['neut']:\n",
    "            if wbc_type in ['NEUTROPHIL']:\n",
    "                label = 'NEUTROPHIL'\n",
    "            #elif wbc_type in ['eosi']:\n",
    "            elif wbc_type in ['EOSINOPHIL']:\n",
    "                label = 'EOSINOPHIL'\n",
    "            #elif wbc_type in ['lymp']:\n",
    "            elif wbc_type in ['LYMPHOCYTE']:\n",
    "                label = 'LYMPHOCYTE'    \n",
    "            #elif wbc_type in ['mono']:\n",
    "            elif wbc_type in ['MONOCYTE']:\n",
    "                label = 'MONOCYTE'\n",
    "            else:\n",
    "                continue\n",
    "            #    label = 'BASOPHIL'\n",
    "            for image_filename in os.listdir(folder + wbc_type):\n",
    "                img_file = cv2.imread(folder + wbc_type + '/' + image_filename)\n",
    "                \n",
    "                if img_file is not None:\n",
    "                    # Downsample the image to 120, 160, 3\n",
    "                    #img_file=skimage.transform.resize(arr=img_file, size=(240, 320, 3))\n",
    "                    img_file=np.array(Image.fromarray(img_file).resize( size=(64, 64)))\n",
    "                    #img_file = scipy.misc.imresize(arr=img_file, size=(64, 64, 3))\n",
    "                    #img_arr = np.asarray(img_file)\n",
    "                    #X.append(img_arr)\n",
    "                    \n",
    "                    ####################################\n",
    "                    # Global Feature extraction\n",
    "                    ####################################\n",
    "                    fv_hu_moments = fd_hu_moments(img_file)\n",
    "                    fv_haralick   = fd_haralick(img_file)\n",
    "                    fv_histogram  = fd_histogram(img_file)\n",
    "                \n",
    "                    hog_features1 = np.array(hog1(img_file))\n",
    "                    ###################################\n",
    "                    # Concatenate global features\n",
    "                    ###################################\n",
    "                    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "                    #global_feature = np.hstack([ hog_features1])\n",
    "                    global_features.append(global_feature)\n",
    "                    y.append(label)\n",
    "    global_features = np.asarray(global_features)\n",
    "    y = np.asarray(y)\n",
    "    return global_features,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baso\t\t   labels.h5\t\t   Testing data\r\n",
      "basotest.bmp\t   lymtest.bmp\t\t   test.png\r\n",
      "bloodcountprog.py  Main_Dataset2\t   testsandeep.png\r\n",
      "contoured111.jpg   Main_Dataset2.zip\t   Untitled Folder\r\n",
      "contoured1.jpg\t   monotest.bmp\t\t   Untitled Folder 2\r\n",
      "data.h5\t\t   neuttest.bmp\t\t   WBC_classification.ipynb\r\n",
      "esotest.bmp\t   scaler.save\t\t   WBC_model_keras.h5\r\n",
      "keras-mlp.csv\t   Segmentation_RBC.ipynb  WBC_model_wieghts.h5\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/feature/_hog.py:239: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9957, 532)\n",
      "<class 'numpy.ndarray'>\n",
      "(9957,)\n",
      "[[ 3.41474503e-01  1.65764324e-03  6.63057296e-03 ...  4.52461766e-25\n",
      "   3.20780222e-17  5.08446759e-25]\n",
      " [ 1.68740943e-01  6.88738562e-03  3.44369281e-03 ... -1.11787848e-26\n",
      "   2.29810487e-18 -1.40647693e-26]\n",
      " [ 3.29328775e-01  6.33324543e-03  9.49986838e-03 ...  1.85174189e-25\n",
      "   1.18201146e-17 -8.51598948e-26]\n",
      " ...\n",
      " [ 3.35756302e-01  5.71230007e-03  3.17350007e-03 ... -9.18014556e-26\n",
      "  -5.16985210e-19  1.08335704e-25]\n",
      " [ 4.42594260e-01  5.08729042e-03  3.81546794e-03 ...  2.11850551e-24\n",
      "   5.11452543e-17 -1.41674536e-24]\n",
      " [ 4.03953135e-01  3.69920442e-03  5.17888647e-03 ...  1.79317312e-25\n",
      "   1.58514058e-17  1.02217495e-25]]\n",
      "['EOSINOPHIL' 'EOSINOPHIL' 'EOSINOPHIL' ... 'NEUTROPHIL' 'NEUTROPHIL'\n",
      " 'NEUTROPHIL']\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "#global_features, labels = get_data('/Main_Dataset 2/')\n",
    "global_features, labels = get_data('/home/sandeep/Sandeep/Python_bloodTest/blood_count/code/clould_code/Main_Dataset2/')\n",
    "\n",
    "#X_test, y_test = get_data('/home/kashish/Documents/Blood_count_debraj_sir/images/TEST_SIMPLE/')\n",
    "print(global_features.shape)\n",
    "print(type(global_features))\n",
    "#print(X_test.shape)\n",
    "print(labels.shape)\n",
    "print(global_features)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] feature vector size (9957, 532)\n",
      "[STATUS] training Labels (9957,)\n",
      "[STATUS] training labels encoded...\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[ 3.09532897e-01  5.59981870e-03  4.76354636e-03  4.41517611e-03\n",
      "  4.97800027e-03  6.77656285e-02  4.90105353e-01  4.28529236e-02\n",
      "  2.82663269e-03  2.38513679e-03  1.76878529e-03  1.62500572e-03\n",
      "  2.23721599e-03  1.34618829e-01  1.12328128e-01  1.21091937e-03\n",
      "  1.72658145e-03  1.59263237e-04  2.68959367e-05  1.51612759e-05\n",
      "  4.04879907e-05  2.65321895e-03  8.13580357e-04  8.36754947e-06\n",
      "  3.88544278e-04  5.27611852e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  9.32519282e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.95230027e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.14068092e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.94032499e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.49117497e-03  2.36751803e-03  2.38297390e-03  2.41684860e-03\n",
      "  2.89062109e-03  1.08829837e-02  4.72953732e-01  7.99026317e-02\n",
      "  6.76256838e-04  3.46182980e-05  3.95369195e-06  1.48179738e-06\n",
      "  1.38149942e-06  5.56288611e-06  1.20667351e-04  1.87909022e-05\n",
      "  4.01324069e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.97533413e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.32550700e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.31224344e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  8.19640470e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.02743602e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.17677951e-04  7.21034393e-04  7.24568585e-04  7.32612976e-04\n",
      "  8.66614136e-04  2.70889203e-03  1.54760337e-01  1.73713835e-02\n",
      "  1.97546438e-04  3.31830914e-06  3.23034068e-07  0.00000000e+00\n",
      "  1.73107269e-07  2.25588870e-05  3.47754516e-05  8.64633186e-07\n",
      "  1.37843811e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.84325509e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.38216704e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.08616406e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  9.52531951e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.40446826e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.93181142e-04  7.16969952e-04  5.45018703e-04  4.74174218e-04\n",
      "  5.38778319e-04  3.09275022e-03  9.04860474e-02  8.40791393e-03\n",
      "  4.72153530e-04  9.07558529e-05  5.14271439e-05  4.59934640e-05\n",
      "  4.96254325e-05  1.25371107e-03  1.67725068e-02  1.57112699e-03\n",
      "  4.34505134e-04  3.12306181e-05  1.58886746e-05  1.22387872e-05\n",
      "  1.66411322e-05  6.47291011e-04  9.23815905e-03  1.66663093e-03\n",
      "  1.17904850e-04  2.10166232e-05  1.41861469e-05  1.54262660e-05\n",
      "  1.56823622e-05  8.65719744e-04  1.23357613e-02  3.12171759e-03\n",
      "  7.21331425e-04  1.43650166e-05  5.73988008e-06  5.11400688e-06\n",
      "  7.22433945e-06  1.25856691e-03  5.08296362e-03  5.85833922e-04\n",
      "  4.81713956e-04  3.32284216e-06  7.46196473e-07  3.30683831e-07\n",
      "  2.35247627e-07  1.57566599e-06  6.63320501e-05  1.71034659e-05\n",
      "  9.08311086e-05  7.81975642e-08  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.07808950e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.45953837e-04  8.82357848e-04  9.49023362e-04  9.88751694e-04\n",
      "  1.28167206e-03  4.94608576e-02  1.29537583e-01  4.61055482e-03\n",
      "  9.14019956e-04  7.51070026e-04  5.19827234e-04  4.46284261e-04\n",
      "  9.69923469e-04  5.81386953e-02  5.34555415e-02  1.51516964e-03\n",
      "  6.50607521e-04  9.89954488e-05  5.24354799e-05  4.92719882e-05\n",
      "  1.95017542e-04  3.99176420e-03  1.95462830e-02  6.38658958e-03\n",
      "  2.22969169e-04  4.73597271e-05  3.86116014e-05  3.74534582e-05\n",
      "  6.36064521e-05  5.23900611e-04  1.76169183e-02  2.73764193e-02\n",
      "  4.33095264e-04  4.20860946e-05  3.19032779e-05  2.97508641e-05\n",
      "  3.63728469e-05  1.21867200e-04  1.47109638e-02  2.23794328e-02\n",
      "  2.70311490e-04  8.70971304e-06  3.94558025e-06  1.75563036e-06\n",
      "  2.46112643e-06  2.25409075e-05  1.39768952e-03  1.20337211e-03\n",
      "  6.68866434e-05  1.55885447e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  7.39689065e-08\n",
      "  3.05539428e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.69481833e-04  1.18677769e-03  1.59020317e-03  1.88965116e-03\n",
      "  2.65577410e-03  1.10254898e-01  1.94669848e-01  4.66362845e-03\n",
      "  1.22406907e-03  1.93315055e-03  1.54387395e-03  1.45772503e-03\n",
      "  2.27998825e-03  1.69320672e-01  7.64059566e-02  6.67646962e-05\n",
      "  8.30396355e-04  1.08409116e-04  5.99143099e-06  1.11884670e-06\n",
      "  8.14686248e-06  1.98059618e-04  1.31679451e-04  0.00000000e+00\n",
      "  2.42554979e-04  2.61239706e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.08600854e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.63166609e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.01453986e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.79289076e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  8.72908933e-03  5.63319339e+02  9.21424935e-01  3.92106212e+03\n",
      "  3.20961041e-01  3.37634072e+02  1.51209292e+04  7.12598026e+00\n",
      "  1.00511548e+01  4.07396751e-04  4.32683523e+00 -3.68775706e-01\n",
      "  9.93939803e-01  8.69810129e-04  9.99909368e-09  1.04440035e-11\n",
      "  4.07211385e-13  2.65291303e-25  7.19445998e-18  5.66419789e-26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29097778 -1.50237731  0.75169629 ...  0.07835741  0.53159751\n",
      "   0.20480385]\n",
      " [-1.28256953  0.49069641 -0.53139522 ... -0.11574198 -0.1046028\n",
      "  -0.03205149]\n",
      " [ 0.1803341   0.27951158  1.90692285 ... -0.03354039  0.09881971\n",
      "  -0.06427902]\n",
      " ...\n",
      " [ 0.2388868   0.04286706 -0.64017936 ... -0.14949401 -0.16474269\n",
      "   0.02343285]\n",
      " [ 1.21214632 -0.19532638 -0.38171231 ...  0.77583322  0.93893843\n",
      "  -0.66788891]\n",
      " [ 0.86013807 -0.72433097  0.16722291 ... -0.03599232  0.1849418\n",
      "   0.02065946]]\n",
      "[STATUS] feature vector normalized...\n",
      "[STATUS] target labels: [0 0 0 ... 3 3 3]\n",
      "[STATUS] target labels shape: (9957,)\n",
      "[STATUS] end of training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandeep/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get the overall feature vector size\n",
    "print (\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
    "\n",
    "# get the overall training label size\n",
    "print (\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
    "\n",
    "# encode the target labels\n",
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(labels)\n",
    "print( \"[STATUS] training labels encoded...\")\n",
    "\n",
    "# normalize the feature vector in the range (0-1)\n",
    "#scaler1 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "\n",
    "#rescaled_features = scaler1.fit_transform(global_features)\n",
    "#print(rescaled_features)\n",
    "#scale_min=scaler1.data_min_\n",
    "#scale_max=scaler1.data_max_\n",
    "#print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "#print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "#scaler_filename = \"scaler.save\"\n",
    "#joblib.dump(scaler1, scaler_filename) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(global_features))\n",
    "print(scaler.mean_)\n",
    "rescaled_features=scaler.transform(global_features)\n",
    "print(rescaled_features)\n",
    "from sklearn.externals import joblib\n",
    "scaler_filename = \"scaler.save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"[STATUS] feature vector normalized...\")\n",
    "\n",
    "print (\"[STATUS] target labels: {}\".format(target))\n",
    "print (\"[STATUS] target labels shape: {}\".format(target.shape))\n",
    "\n",
    "# save the feature vector using HDF5\n",
    "#SSSS h5f_data = h5py.File('/resources/data/output/data.h5', 'w')\n",
    "h5f_data = h5py.File('data.h5', 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
    "\n",
    "h5f_label = h5py.File('labels.h5', 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "print( \"[STATUS] end of training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] features shape: (9957, 532)\n",
      "[STATUS] labels shape: (9957,)\n",
      "[STATUS] training started...\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# create all the machine learning models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state=9)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier(random_state=9)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=9)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=9)))\n",
    "models.append(('MLP',MLPClassifier(hidden_layer_sizes=(100,100), max_iter=200, alpha=0.0001,\n",
    "                     solver='adam',random_state=21,tol=0.000000001)))\n",
    "models.append(('MLP1',MLPClassifier(hidden_layer_sizes=(13), max_iter=200, alpha=0.0001,\n",
    "                     solver='adam',random_state=21,tol=0.000000001)))\n",
    "\n",
    "\n",
    "\n",
    "# variables to hold the results and names\n",
    "results = []\n",
    "names = []\n",
    "scoring = \"accuracy\"\n",
    "\n",
    "# import the feature vector and trained labels\n",
    "h5f_data = h5py.File('data.h5', 'r')\n",
    "h5f_label = h5py.File('labels.h5', 'r')\n",
    "\n",
    "global_features_string = h5f_data['dataset_1']\n",
    "global_labels_string = h5f_label['dataset_1']\n",
    "\n",
    "global_features = np.array(global_features_string)\n",
    "global_labels = np.array(global_labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "# verify the shape of the feature vector and labels\n",
    "print (\"[STATUS] features shape: {}\".format(global_features.shape))\n",
    "print (\"[STATUS] labels shape: {}\".format(global_labels.shape))\n",
    "\n",
    "print (\"[STATUS] training started...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] splitted train and test data...\n",
      "Train data  : (7965, 532)\n",
      "Test data   : (1992, 532)\n",
      "Train labels: (7965,)\n",
      "Test labels : (1992,)\n"
     ]
    }
   ],
   "source": [
    "# split the training and testing data\n",
    "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
    "                                                                                          np.array(global_labels),\n",
    "                                                                                          test_size=test_size,\n",
    "                                                                                          random_state=seed)\n",
    "\n",
    "print (\"[STATUS] splitted train and test data...\")\n",
    "print (\"Train data  : {}\".format(trainDataGlobal.shape))\n",
    "print (\"Test data   : {}\".format(testDataGlobal.shape))\n",
    "print (\"Train labels: {}\".format(trainLabelsGlobal.shape))\n",
    "print (\"Test labels : {}\".format(testLabelsGlobal.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.937856 (0.008629)\n",
      "LDA: 0.886884 (0.015221)\n",
      "KNN: 0.890396 (0.011777)\n",
      "CART: 0.964721 (0.006792)\n",
      "RF: 0.994352 (0.002819)\n",
      "NB: 0.630137 (0.014176)\n",
      "SVM: 0.947647 (0.008761)\n",
      "MLP: 0.965726 (0.005610)\n",
      "MLP1: 0.971878 (0.005265)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 40s, sys: 19min 58s, total: 1h 8min 39s\n",
      "Wall time: 27min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# filter all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 10-fold cross validation\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Machine Learning algorithm comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: np_utils in /home/sandeep/.local/lib/python3.5/site-packages (0.5.12.1)\n",
      "Requirement already satisfied: future>=0.16 in /home/sandeep/.local/lib/python3.5/site-packages (from np_utils) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.0 in /home/sandeep/.local/lib/python3.5/site-packages (from np_utils) (1.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Training...\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 7168 samples, validate on 797 samples\n",
      "Epoch 1/20\n",
      "7168/7168 [==============================] - 2s 244us/sample - loss: 0.5868 - val_loss: 0.2460\n",
      "Epoch 2/20\n",
      "7168/7168 [==============================] - 1s 167us/sample - loss: 0.2101 - val_loss: 0.1650\n",
      "Epoch 3/20\n",
      "7168/7168 [==============================] - 1s 169us/sample - loss: 0.1250 - val_loss: 0.1323\n",
      "Epoch 4/20\n",
      "7168/7168 [==============================] - 1s 172us/sample - loss: 0.0998 - val_loss: 0.1029\n",
      "Epoch 5/20\n",
      "7168/7168 [==============================] - 1s 174us/sample - loss: 0.0639 - val_loss: 0.1069\n",
      "Epoch 6/20\n",
      "7168/7168 [==============================] - 1s 171us/sample - loss: 0.0475 - val_loss: 0.0927\n",
      "Epoch 7/20\n",
      "7168/7168 [==============================] - 1s 169us/sample - loss: 0.0462 - val_loss: 0.1018\n",
      "Epoch 8/20\n",
      "7168/7168 [==============================] - 1s 172us/sample - loss: 0.0361 - val_loss: 0.0741\n",
      "Epoch 9/20\n",
      "7168/7168 [==============================] - 1s 173us/sample - loss: 0.0309 - val_loss: 0.0721\n",
      "Epoch 10/20\n",
      "7168/7168 [==============================] - 1s 173us/sample - loss: 0.0280 - val_loss: 0.1102\n",
      "Epoch 11/20\n",
      "7168/7168 [==============================] - 1s 191us/sample - loss: 0.0250 - val_loss: 0.0938\n",
      "Epoch 12/20\n",
      "7168/7168 [==============================] - 2s 234us/sample - loss: 0.0196 - val_loss: 0.0849\n",
      "Epoch 13/20\n",
      "7168/7168 [==============================] - 1s 171us/sample - loss: 0.0235 - val_loss: 0.1215\n",
      "Epoch 14/20\n",
      "7168/7168 [==============================] - 1s 172us/sample - loss: 0.0467 - val_loss: 0.0782\n",
      "Epoch 15/20\n",
      "7168/7168 [==============================] - 1s 170us/sample - loss: 0.0212 - val_loss: 0.0772\n",
      "Epoch 16/20\n",
      "7168/7168 [==============================] - 1s 172us/sample - loss: 0.0227 - val_loss: 0.0693\n",
      "Epoch 17/20\n",
      "7168/7168 [==============================] - 1s 172us/sample - loss: 0.0214 - val_loss: 0.1098\n",
      "Epoch 18/20\n",
      "7168/7168 [==============================] - 1s 174us/sample - loss: 0.0159 - val_loss: 0.0626\n",
      "Epoch 19/20\n",
      "7168/7168 [==============================] - 1s 172us/sample - loss: 0.0119 - val_loss: 0.0820\n",
      "Epoch 20/20\n",
      "7168/7168 [==============================] - 1s 173us/sample - loss: 0.0184 - val_loss: 0.0732\n",
      "Generating test predictions...\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 3\n",
      "0 : 0\n",
      "3 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "0 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "0 : 0\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "3 : 3\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "0 : 0\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "0 : 0\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "3 : 3\n",
      "3 : 3\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "2 : 2\n",
      "0 : 0\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "1 : 1\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "3 : 3\n",
      "2 : 2\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 0\n",
      "2 : 2\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "3 : 3\n",
      "1 : 1\n",
      "1 : 1\n",
      "3 : 3\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "2 : 2\n",
      "3 : 3\n",
      "0 : 0\n",
      "2 : 2\n",
      "0 : 0\n",
      "1 : 1\n",
      "2 : 2\n",
      "1 : 1\n",
      "3 : 3\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "0 : 0\n",
      "1 : 1\n",
      "0.9748995983935743\n"
     ]
    }
   ],
   "source": [
    "!pip install np_utils\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.np_utils import np_utils\n",
    "#from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.np_utils import np_utils\n",
    "#from keras.layers.core import Dense, Activation, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal\n",
    "\n",
    "#y_train = np_utils.to_categorical(trainLabelsGlobal) \n",
    "y_train = to_categorical(trainLabelsGlobal) \n",
    "X_train=trainDataGlobal\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.add(Dense(30, input_dim=input_dim))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.15))\n",
    "#model.add(Dense(13))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.15))\n",
    "#model.add(Dense(nb_classes))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print(\"Training...\")\n",
    "history=model.fit(X_train, y_train, nb_epoch=20, batch_size=16, validation_split=0.1)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(testDataGlobal, verbose=0)\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    print(testLabelsGlobal[i],\":\",preds[i]) \n",
    "\n",
    "print(accuracy_score(testLabelsGlobal, preds))\n",
    "\n",
    "def write_preds(preds, fname):\n",
    "    pd.DataFrame({\"ImageId\": list(range(1,len(preds)+1)), \"Label\": preds}).to_csv(fname, index=False, header=True)\n",
    "\n",
    "write_preds(preds, \"keras-mlp.csv\")\n",
    "\n",
    "model.save_weights('WBC_model_wieghts.h5')\n",
    "model.save('WBC_model_keras.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_prediction(img_file):\n",
    "    \"\"\"\n",
    "    Load the data and labels from the given folder.\n",
    "    \"\"\"\n",
    "    global_features = []\n",
    "    y = []\n",
    "    \n",
    "    if img_file is not None:\n",
    "        # Downsample the image to 120, 160, 3\n",
    "        #img_file=skimage.transform.resize(arr=img_file, size=(240, 320, 3))\n",
    "        img_file=np.array(Image.fromarray(img_file).resize(size=(64, 64)))\n",
    "       # img_file = scipy.misc.imresize(arr=img_file, size=(64, 64, 3))\n",
    "        #img_arr = np.asarray(img_file)\n",
    "        #X.append(img_arr)\n",
    "                    \n",
    "        ####################################\n",
    "        # Global Feature extraction\n",
    "        ####################################\n",
    "        fv_hu_moments = fd_hu_moments(img_file)\n",
    "        fv_haralick   = fd_haralick(img_file)\n",
    "        fv_histogram  = fd_histogram(img_file)\n",
    "                \n",
    "        hog_features1 = np.array(hog1(img_file))\n",
    "        ###################################\n",
    "        # Concatenate global features\n",
    "        ###################################\n",
    "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "        #global_feature = np.hstack([ hog_features1])\n",
    "        global_features.append(global_feature)\n",
    "      \n",
    "                \n",
    "    global_features = np.asarray(global_features)\n",
    "    \n",
    "    return global_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "WBC TYPE: NEUTROPHIL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = cv2.imread('esotest.bmp')\n",
    "\n",
    "def WBC_classification(img):\n",
    "    from tensorflow.keras.models import load_model\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    model = load_model('WBC_model_keras.h5')\n",
    "   \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    model.load_weights('WBC_model_wieghts.h5')\n",
    "    \n",
    "    global_features=get_data_for_prediction(img)\n",
    "    #print(global_features)\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    scaler1 = joblib.load(\"scaler.save\") \n",
    "    #print(scaler1.data_max_)\n",
    "    \n",
    "    #scale_min=scaler.data_min_\n",
    "    #scale_max=scaler.data_max_\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #X_std=(global_features-scale_min)/(scale_max-scale_min)\n",
    "    #rescaled_features=X_std*(1-0)+0\n",
    "    rescaled_features = scaler1.transform(global_features)\n",
    "    #img = cv2.resize(img,(64,64))\n",
    "    #img = np.reshape(img,[1,64,64,3])\n",
    "    #print(rescaled_features)\n",
    "    classes = model.predict_classes(rescaled_features)\n",
    "    #if classes[0]==0:\n",
    "    #    label=\"BASOPHIL\"\n",
    "    if classes[0]==0:\n",
    "        label=\"EOSINOPHIL\"\n",
    "    elif classes[0]==1:\n",
    "        label=\"LYMPHOCYTE\"\n",
    "    elif classes[0]==2:\n",
    "        label=\"MONOCYTES\"\n",
    "    else:\n",
    "        label=\"NEUTROPHIL\"\n",
    "\n",
    "    print(classes)\n",
    "    #print (type(classes))\n",
    "    return label\n",
    "    \n",
    "wbc_label=WBC_classification(img)\n",
    "print(\"WBC TYPE:\",wbc_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-57d4de472996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstepSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_height\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# window size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_width\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#Sliding Windows for Object Detection with Python\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# read the image and define the stepSize and window size \n",
    "# (width,height)\n",
    "image = cv2.imread(\"cell.png\") # your image path\n",
    "tmp = image # for drawing a rectangle\n",
    "stepSize = 50\n",
    "(w_width, w_height) = (50, 50) # window size\n",
    "for x in range(0, image.shape[1] - w_width , stepSize):\n",
    "    for y in range(0, image.shape[0] - w_height, stepSize):\n",
    "        window = image[x:x + w_width, y:y + w_height, :]\n",
    "       \n",
    "#classify content of the window with your classifier and  \n",
    "# determine if the window includes an object (cell) or not\n",
    "\n",
    "# draw window on image\n",
    "cv2.rectangle(tmp, (x, y), (x + w_width, y + w_height), (255, 0, 0), 2) # draw rectangle on image\n",
    "plt.imshow(np.array(tmp).astype('uint8'))\n",
    "# show all windows\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
